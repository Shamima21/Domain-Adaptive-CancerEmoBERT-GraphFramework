{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKzL-JbgZgYh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ONE-DATASET (Sadness) RUN: Embeddings + GNN + Text + Ensemble+\n",
        "# +  GNN Explainer with visualization\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install torch-geometric faiss-cpu sentence-transformers transformers accelerate scikit-learn networkx matplotlib\n",
        "\n",
        "import os, random, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "\n",
        "# -------------------------\n",
        "# 0) Reproducibility\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "def cleanup_cuda():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------\n",
        "# 1) CONFIG (Sadness only)\n",
        "# -------------------------\n",
        "CSV_PATH  = \"/content/sample_data/Surprise_anon.csv\"\n",
        "LABEL_COL = \"Surprise\"\n",
        "TEXT_COL  = \"Sentence\"\n",
        "SPLIT_COL = \"Split\"     # 0=train, 1=val, 2=test\n",
        "\n",
        "# Graph hyperparams\n",
        "K   = 10\n",
        "THR = 0.60\n",
        "\n",
        "# Models\n",
        "SENT_MODEL      = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
        "BASE_TEXT_MODEL = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "\n",
        "# GNN training hyperparams\n",
        "GNN_HID      = 256\n",
        "GNN_DROPOUT  = 0.5\n",
        "GNN_LR       = 5e-4\n",
        "GNN_WD       = 5e-4\n",
        "GNN_EPOCHS   = 300\n",
        "GNN_PATIENCE = 25\n",
        "\n",
        "# Text training hyperparams (keep yours; can change later)\n",
        "TEXT_EPOCHS  = 4\n",
        "TEXT_BS      = 16\n",
        "TEXT_LR      = 3e-5\n",
        "TEXT_WD      = 0.01\n",
        "MAX_LEN      = 128\n",
        "\n",
        "# Explainer hyperparams\n",
        "EXPL_NUM_HOPS = 2\n",
        "EXPL_EPOCHS   = 200\n",
        "EXPL_LR       = 0.05\n",
        "EXPL_LAM_SIZE = 0.01\n",
        "EXPL_LAM_ENT  = 0.001\n",
        "EXPL_TOP_EDGES= 25\n",
        "\n",
        "# -------------------------\n",
        "# 2) Helpers\n",
        "# -------------------------\n",
        "def acc_from_probs(p, ytrue):\n",
        "    pred = p.argmax(axis=1)\n",
        "    return float((pred == ytrue).mean())\n",
        "\n",
        "def metrics_from_probs(p, ytrue):\n",
        "    pred = p.argmax(axis=1)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(ytrue, pred)),\n",
        "        \"f1\": float(f1_score(ytrue, pred, zero_division=0)),\n",
        "        \"precision\": float(precision_score(ytrue, pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(ytrue, pred, zero_division=0)),\n",
        "    }\n",
        "\n",
        "def make_class_weights(y, train_mask):\n",
        "    y_train = np.array(y)[train_mask]\n",
        "    pos = int((y_train == 1).sum())\n",
        "    neg = int((y_train == 0).sum())\n",
        "    if pos == 0 or neg == 0:\n",
        "        return torch.tensor([1.0, 1.0], dtype=torch.float, device=device)\n",
        "    w0 = (pos + neg) / (2.0 * neg)\n",
        "    w1 = (pos + neg) / (2.0 * pos)\n",
        "    return torch.tensor([w0, w1], dtype=torch.float, device=device)\n",
        "\n",
        "class WeightedGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hid=256, num_classes=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.c1 = GCNConv(in_dim, hid)\n",
        "        self.c2 = GCNConv(hid, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.c1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.c2(x, edge_index, edge_weight)\n",
        "        return x\n",
        "\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc, y):\n",
        "        self.enc = enc\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(int(self.y[idx]))\n",
        "        return item\n",
        "\n",
        "def comp_metrics_hf(eval_pred):\n",
        "    logits, labels_ = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels_, preds),\n",
        "        \"f1\": f1_score(labels_, preds, zero_division=0),\n",
        "        \"precision\": precision_score(labels_, preds, zero_division=0),\n",
        "        \"recall\": recall_score(labels_, preds, zero_division=0),\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# 3) Load shared models\n",
        "# -------------------------\n",
        "sent_model = SentenceTransformer(SENT_MODEL, device=device)\n",
        "tok = AutoTokenizer.from_pretrained(BASE_TEXT_MODEL)\n",
        "\n",
        "def tokenize_list(text_list, max_len=128):\n",
        "    return tok(text_list, truncation=True, padding=True, max_length=max_len)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Load dataset\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET: Surprise\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = df[[TEXT_COL, LABEL_COL, SPLIT_COL]].dropna().reset_index(drop=True)\n",
        "\n",
        "texts  = df[TEXT_COL].astype(str).tolist()\n",
        "labels = df[LABEL_COL].astype(int).tolist()\n",
        "splits = df[SPLIT_COL].astype(int).tolist()\n",
        "\n",
        "splits_np = np.array(splits)\n",
        "train_mask = splits_np == 0\n",
        "val_mask   = splits_np == 1\n",
        "test_mask  = splits_np == 2\n",
        "\n",
        "print(\"Total:\", len(df))\n",
        "print(\"Train/Val/Test:\", int(train_mask.sum()), int(val_mask.sum()), int(test_mask.sum()))\n",
        "print(\"Train label counts:\", df[df[SPLIT_COL]==0][LABEL_COL].value_counts().to_dict())\n",
        "\n",
        "# -------------------------\n",
        "# 5) Sentence embeddings for graph features\n",
        "# -------------------------\n",
        "X = sent_model.encode(\n",
        "    texts,\n",
        "    batch_size=64,\n",
        "    convert_to_tensor=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "X_np = X.detach().cpu().numpy().astype(\"float32\")\n",
        "\n",
        "# -------------------------\n",
        "# 6) Leakage-free kNN graph\n",
        "#    - build faiss index on TRAIN+VAL only\n",
        "#    - connect TEST -> TRAIN+VAL only\n",
        "# -------------------------\n",
        "idx_trainval = np.where(splits_np != 2)[0]\n",
        "idx_test     = np.where(splits_np == 2)[0]\n",
        "\n",
        "index = faiss.IndexFlatIP(X_np.shape[1])\n",
        "index.add(X_np[idx_trainval])\n",
        "\n",
        "edge_src, edge_dst, edge_wt = [], [], []\n",
        "\n",
        "# (A) edges among train+val\n",
        "sims, nbrs = index.search(X_np[idx_trainval], K+1)\n",
        "for local_i, i in enumerate(idx_trainval):\n",
        "    for j in range(1, K+1):\n",
        "        nb_local = int(nbrs[local_i, j])\n",
        "        nb = int(idx_trainval[nb_local])\n",
        "        w = float(sims[local_i, j])\n",
        "        if w >= THR:\n",
        "            edge_src.append(i); edge_dst.append(nb); edge_wt.append(w)\n",
        "\n",
        "# (B) test -> train+val\n",
        "sims_t, nbrs_t = index.search(X_np[idx_test], K)\n",
        "for local_i, i in enumerate(idx_test):\n",
        "    for j in range(K):\n",
        "        nb_local = int(nbrs_t[local_i, j])\n",
        "        nb = int(idx_trainval[nb_local])\n",
        "        w = float(sims_t[local_i, j])\n",
        "        if w >= THR:\n",
        "            edge_src.append(i); edge_dst.append(nb); edge_wt.append(w)\n",
        "\n",
        "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
        "edge_weight = torch.tensor(edge_wt, dtype=torch.float)\n",
        "\n",
        "# make undirected\n",
        "rev_edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)\n",
        "edge_index = torch.cat([edge_index, rev_edge_index], dim=1)\n",
        "edge_weight = torch.cat([edge_weight, edge_weight.clone()], dim=0)\n",
        "\n",
        "print(\"Undirected edges:\", int(edge_index.shape[1]))\n",
        "\n",
        "# -------------------------\n",
        "# 7) PyG data\n",
        "# -------------------------\n",
        "y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "data = Data(\n",
        "    x=X.detach().cpu(),\n",
        "    edge_index=edge_index,\n",
        "    edge_weight=edge_weight,\n",
        "    y=y,\n",
        "    train_mask=torch.tensor(train_mask),\n",
        "    val_mask=torch.tensor(val_mask),\n",
        "    test_mask=torch.tensor(test_mask),\n",
        ").to(device)\n",
        "\n",
        "class_weights = make_class_weights(labels, train_mask)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(\"Class weights:\", class_weights.detach().cpu().tolist())\n",
        "\n",
        "# -------------------------\n",
        "# 8) Train GNN\n",
        "# -------------------------\n",
        "gnn = WeightedGCN(in_dim=data.x.size(1), hid=GNN_HID, num_classes=2, dropout=GNN_DROPOUT).to(device)\n",
        "opt = torch.optim.AdamW(gnn.parameters(), lr=GNN_LR, weight_decay=GNN_WD)\n",
        "\n",
        "def gnn_train(max_epochs=300, patience=25):\n",
        "    best_val = -1\n",
        "    best_state = None\n",
        "    bad = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        gnn.train()\n",
        "        opt.zero_grad()\n",
        "        logits = gnn(data.x, data.edge_index, data.edge_weight)\n",
        "        loss = criterion(logits[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gnn.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            gnn.eval()\n",
        "            pred = logits.argmax(dim=1)\n",
        "            tr = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()\n",
        "            va = (pred[data.val_mask]   == data.y[data.val_mask]).float().mean().item()\n",
        "            te = (pred[data.test_mask]  == data.y[data.test_mask]).float().mean().item()\n",
        "            print(f\"GNN Epoch {epoch:03d} | loss {loss.item():.4f} | train {tr:.3f} | val {va:.3f} | test {te:.3f}\")\n",
        "\n",
        "            if va > best_val:\n",
        "                best_val = va\n",
        "                best_state = {k: v.detach().cpu().clone() for k, v in gnn.state_dict().items()}\n",
        "                bad = 0\n",
        "            else:\n",
        "                bad += 1\n",
        "                if bad >= patience:\n",
        "                    print(f\"GNN Early stop at epoch {epoch} (best val={best_val:.3f})\")\n",
        "                    break\n",
        "\n",
        "    if best_state is not None:\n",
        "        gnn.load_state_dict(best_state)\n",
        "    return float(best_val)\n",
        "\n",
        "best_val_gnn = gnn_train(max_epochs=GNN_EPOCHS, patience=GNN_PATIENCE)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gnn_probs(mask_tensor):\n",
        "    gnn.eval()\n",
        "    logits = gnn(data.x, data.edge_index, data.edge_weight)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    return probs[mask_tensor].detach().cpu().numpy()\n",
        "\n",
        "p_gnn_val  = gnn_probs(data.val_mask)\n",
        "p_gnn_test = gnn_probs(data.test_mask)\n",
        "\n",
        "y_val  = df[val_mask][LABEL_COL].astype(int).to_numpy()\n",
        "y_test = df[test_mask][LABEL_COL].astype(int).to_numpy()\n",
        "\n",
        "gnn_test_metrics = metrics_from_probs(p_gnn_test, y_test)\n",
        "print(\"\\nGNN metrics (test):\", gnn_test_metrics)\n",
        "\n",
        "# -------------------------\n",
        "# 9) Train Text model (BioBERT)\n",
        "# -------------------------\n",
        "X_train = df[train_mask][TEXT_COL].tolist()\n",
        "y_train = df[train_mask][LABEL_COL].astype(int).tolist()\n",
        "X_val   = df[val_mask][TEXT_COL].tolist()\n",
        "y_val_l = df[val_mask][LABEL_COL].astype(int).tolist()\n",
        "X_test  = df[test_mask][TEXT_COL].tolist()\n",
        "y_test_l= df[test_mask][LABEL_COL].astype(int).tolist()\n",
        "\n",
        "train_ds = SimpleDataset(tokenize_list(X_train, MAX_LEN), y_train)\n",
        "val_ds   = SimpleDataset(tokenize_list(X_val,   MAX_LEN), y_val_l)\n",
        "test_ds  = SimpleDataset(tokenize_list(X_test,  MAX_LEN), y_test_l)\n",
        "\n",
        "text_model = AutoModelForSequenceClassification.from_pretrained(BASE_TEXT_MODEL, num_labels=2)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./tmp_text_sadness\",\n",
        "    num_train_epochs=TEXT_EPOCHS,\n",
        "    per_device_train_batch_size=TEXT_BS,\n",
        "    per_device_eval_batch_size=TEXT_BS,\n",
        "    learning_rate=TEXT_LR,\n",
        "    weight_decay=TEXT_WD,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=text_model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=comp_metrics_hf,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "@torch.no_grad()\n",
        "def text_probs(dataset):\n",
        "    preds = trainer.predict(dataset)\n",
        "    logits = preds.predictions\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "p_text_val  = text_probs(val_ds)\n",
        "p_text_test = text_probs(test_ds)\n",
        "\n",
        "text_test_metrics = metrics_from_probs(p_text_test, y_test)\n",
        "print(\"\\nText metrics (test):\", text_test_metrics)\n",
        "\n",
        "# -------------------------\n",
        "# 10) Ensemble (alpha tuned on VAL)\n",
        "# -------------------------\n",
        "best_alpha = None\n",
        "best_val_acc = -1\n",
        "best_val_metrics = None\n",
        "best_test_metrics = None\n",
        "\n",
        "for alpha in [i/10 for i in range(11)]:\n",
        "    p_ens_val  = alpha * p_text_val  + (1 - alpha) * p_gnn_val\n",
        "    val_acc = acc_from_probs(p_ens_val, y_val)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_alpha = alpha\n",
        "        best_val_metrics  = metrics_from_probs(p_ens_val,  y_val)\n",
        "\n",
        "        p_ens_test = alpha * p_text_test + (1 - alpha) * p_gnn_test\n",
        "        best_test_metrics = metrics_from_probs(p_ens_test, y_test)\n",
        "\n",
        "print(\"\\n---------------- RESULTS (Sadness) ----------------\")\n",
        "print(f\"GNN best val acc: {best_val_gnn:.3f}\")\n",
        "print(\"GNN  (test):\", gnn_test_metrics)\n",
        "print(\"Text (test):\", text_test_metrics)\n",
        "print(f\"Ensemble best alpha: {best_alpha}\")\n",
        "print(\"Ensemble (val):\", best_val_metrics)\n",
        "print(\"Ensemble(test):\", best_test_metrics)\n",
        "\n",
        "# ============================================================\n",
        "# 11) STABLE RexYing-style Explainer (edge mask)\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def pick_test_node_prefer_correct(data, logits):\n",
        "    pred = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "    y    = data.y.detach().cpu().numpy()\n",
        "    test_ids = torch.where(data.test_mask)[0].detach().cpu().numpy().tolist()\n",
        "    correct = [i for i in test_ids if pred[i] == y[i]]\n",
        "    return int(correct[0]) if len(correct) > 0 else int(test_ids[0])\n",
        "\n",
        "def explain_node_edge_mask_stable(\n",
        "    model,\n",
        "    data,\n",
        "    node_idx,\n",
        "    num_hops=2,\n",
        "    epochs=200,\n",
        "    lr=0.05,\n",
        "    lam_size=0.01,\n",
        "    lam_ent=0.001,\n",
        "    top_edges=25,\n",
        "    eps=1e-9,\n",
        "):\n",
        "    \"\"\"\n",
        "    Learns an edge mask for a local k-hop subgraph around node_idx.\n",
        "    Robust to:\n",
        "      - empty subgraphs (adds self-loop)\n",
        "      - NaNs (entropy stability + clamping)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    subset, edge_index_sub, mapping, edge_id = k_hop_subgraph(\n",
        "        node_idx=int(node_idx),\n",
        "        num_hops=int(num_hops),\n",
        "        edge_index=data.edge_index,\n",
        "        relabel_nodes=True,\n",
        "        num_nodes=data.num_nodes,\n",
        "        flow=\"source_to_target\",\n",
        "    )\n",
        "\n",
        "    subset_orig = subset.detach().cpu().numpy()\n",
        "    center_local = int(mapping.item())\n",
        "\n",
        "    x_sub = data.x[subset]\n",
        "\n",
        "    # subgraph edge weights\n",
        "    if data.edge_weight is None:\n",
        "        base_ew = torch.ones(edge_index_sub.size(1), device=data.x.device)\n",
        "    else:\n",
        "        base_ew = data.edge_weight[edge_id].clone()\n",
        "\n",
        "    # If no edges, add a self-loop so explainer has something to optimize\n",
        "    if edge_index_sub.numel() == 0 or edge_index_sub.size(1) == 0:\n",
        "        edge_index_sub = torch.tensor([[center_local],[center_local]],\n",
        "                                      device=data.x.device, dtype=torch.long)\n",
        "        base_ew = torch.ones(1, device=data.x.device)\n",
        "\n",
        "    base_ew = torch.nan_to_num(base_ew, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "    base_ew = torch.clamp(base_ew, min=0.0, max=1.0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_clean = model(x_sub, edge_index_sub, base_ew)  # [subN,2]\n",
        "        target_class = int(out_clean[center_local].argmax().item())\n",
        "\n",
        "    mask_logits = torch.nn.Parameter(torch.zeros(edge_index_sub.size(1), device=data.x.device))\n",
        "    optm = torch.optim.Adam([mask_logits], lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        optm.zero_grad()\n",
        "        mask = torch.sigmoid(mask_logits)\n",
        "        masked_ew = base_ew * mask\n",
        "\n",
        "        out = model(x_sub, edge_index_sub, masked_ew)\n",
        "\n",
        "        # scalar loss (center node)\n",
        "        logp = F.log_softmax(out[center_local], dim=-1)   # [2]\n",
        "        loss_pred = -logp[target_class]\n",
        "\n",
        "        loss_size = lam_size * mask.sum()\n",
        "\n",
        "        m = torch.clamp(mask, eps, 1 - eps)\n",
        "        ent = -(m * torch.log(m) + (1 - m) * torch.log(1 - m))\n",
        "        loss_ent = lam_ent * ent.mean()\n",
        "\n",
        "        loss = loss_pred + loss_size + loss_ent\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            loss = loss_pred\n",
        "\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "        if ep % 50 == 0 or ep == 1:\n",
        "            with torch.no_grad():\n",
        "                prob_t = torch.softmax(out[center_local], dim=-1)[target_class].item()\n",
        "                print(f\"Explainer ep {ep:03d} | loss={float(loss.item()):.4f} | target_prob={prob_t:.4f} | mask_mean={float(mask.mean().item()):.3f}\")\n",
        "\n",
        "    edge_mask = torch.sigmoid(mask_logits).detach().cpu().numpy()\n",
        "    order = np.argsort(-edge_mask)\n",
        "    top_keep_idx = order[:min(top_edges, len(order))]\n",
        "\n",
        "    return subset_orig, edge_index_sub.detach().cpu(), edge_mask, center_local, target_class, top_keep_idx\n",
        "\n",
        "def plot_explanation_q1(\n",
        "    subset_orig,\n",
        "    edge_index_sub,\n",
        "    edge_mask,\n",
        "    center_local,\n",
        "    top_keep_idx,\n",
        "    true_full,\n",
        "    pred_full,\n",
        "    node_idx_global,\n",
        "    sentence_text,\n",
        "    save_path=\"sadness_explainer_q1.png\"\n",
        "):\n",
        "    src = edge_index_sub[0].numpy()\n",
        "    dst = edge_index_sub[1].numpy()\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    nodes_in_plot = set([center_local])\n",
        "\n",
        "    for ei in top_keep_idx:\n",
        "        u = int(src[ei]); v = int(dst[ei])\n",
        "        imp = float(edge_mask[ei])\n",
        "        if imp <= 1e-6:\n",
        "            continue\n",
        "        G.add_edge(u, v, imp=imp)\n",
        "        nodes_in_plot.add(u); nodes_in_plot.add(v)\n",
        "\n",
        "    for n in nodes_in_plot:\n",
        "        G.add_node(n)\n",
        "\n",
        "    # fallback: ensure at least one edge is plotted\n",
        "    if G.number_of_edges() == 0:\n",
        "        best_imp = float(np.max(edge_mask)) if len(edge_mask) else 1.0\n",
        "        G.add_edge(center_local, center_local, imp=best_imp)\n",
        "\n",
        "    node_true = {n: int(true_full[int(subset_orig[n])]) for n in G.nodes()}\n",
        "    node_pred = {n: int(pred_full[int(subset_orig[n])]) for n in G.nodes()}\n",
        "\n",
        "    sizes = [1400 if n == center_local else 850 for n in G.nodes()]\n",
        "    node_colors = [node_true[n] for n in G.nodes()]\n",
        "\n",
        "    imps = np.array([G[u][v][\"imp\"] for u, v in G.edges()])\n",
        "    if imps.max() - imps.min() < 1e-9:\n",
        "        widths = np.ones_like(imps) * 2.5\n",
        "    else:\n",
        "        widths = 0.8 + 5.2 * (imps - imps.min()) / (imps.max() - imps.min())\n",
        "\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.7)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors, cmap=plt.cm.Set2)\n",
        "\n",
        "    # border style encodes prediction\n",
        "    ax = plt.gca()\n",
        "    nodes_list = list(G.nodes())\n",
        "    for n in nodes_list:\n",
        "        x0, y0 = pos[n]\n",
        "        ls = \"--\" if node_pred[n] == 1 else \"-\"\n",
        "        ax.scatter([x0], [y0],\n",
        "                   s=(sizes[nodes_list.index(n)] * 1.05),\n",
        "                   facecolors=\"none\",\n",
        "                   edgecolors=\"black\",\n",
        "                   linewidths=2,\n",
        "                   linestyles=ls)\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos, width=widths, arrows=True, arrowstyle=\"-|>\", arrowsize=12, alpha=0.9)\n",
        "    labels_local = {n: str(int(subset_orig[n])) for n in G.nodes()}\n",
        "    nx.draw_networkx_labels(G, pos, labels=labels_local, font_size=9)\n",
        "\n",
        "    plt.title(f\"Local GNN Explanation (Sadness) | test node={node_idx_global}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nSaved figure: {save_path}\")\n",
        "    print(\"\\nSentence (explained node):\")\n",
        "    print(sentence_text)\n",
        "\n",
        "    print(\"\\nCaption (paste-ready):\")\n",
        "    print(\n",
        "        \"Local explanation of the GNN prediction for a representative test instance in the Sadness dataset. \"\n",
        "        \"A k-hop ego-subgraph is extracted around the target node, and an edge-importance mask is learned by \"\n",
        "        \"optimizing prediction fidelity with sparsity and entropy regularization (RexYing-style explainer). \"\n",
        "        \"Only the highest-importance edges are visualized; thicker arrows indicate higher importance. \"\n",
        "        \"Node color denotes the ground-truth label (0/1), while border style denotes the predicted label \"\n",
        "        \"(solid: predicted 0; dashed: predicted 1).\"\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# 12) Run explainer + plot\n",
        "# -------------------------\n",
        "gnn.eval()\n",
        "with torch.no_grad():\n",
        "    logits_full = gnn(data.x, data.edge_index, data.edge_weight)\n",
        "\n",
        "true_full = data.y.detach().cpu().numpy()\n",
        "pred_full = logits_full.argmax(dim=1).detach().cpu().numpy()\n",
        "\n",
        "node_idx = pick_test_node_prefer_correct(data, logits_full)\n",
        "print(f\"\\n[Explainer] Using TEST node={node_idx} (correct preferred).\")\n",
        "print(\"Sentence:\", texts[node_idx])\n",
        "\n",
        "subset_orig, edge_index_sub, edge_mask, center_local, target_class, top_keep_idx = explain_node_edge_mask_stable(\n",
        "    model=gnn,\n",
        "    data=data,\n",
        "    node_idx=node_idx,\n",
        "    num_hops=EXPL_NUM_HOPS,     # if too sparse, set 3\n",
        "    epochs=EXPL_EPOCHS,\n",
        "    lr=EXPL_LR,\n",
        "    lam_size=EXPL_LAM_SIZE,     # if no edges selected, reduce to 0.001\n",
        "    lam_ent=EXPL_LAM_ENT,\n",
        "    top_edges=EXPL_TOP_EDGES,\n",
        ")\n",
        "\n",
        "plot_explanation_q1(\n",
        "    subset_orig=subset_orig,\n",
        "    edge_index_sub=edge_index_sub,\n",
        "    edge_mask=edge_mask,\n",
        "    center_local=center_local,\n",
        "    top_keep_idx=top_keep_idx,\n",
        "    true_full=true_full,\n",
        "    pred_full=pred_full,\n",
        "    node_idx_global=node_idx,\n",
        "    sentence_text=texts[node_idx],\n",
        "    save_path=\"sadness_explainer_q1.png\"\n",
        ")\n",
        "\n",
        "cleanup_cuda()\n"
      ]
    }
  ]
}