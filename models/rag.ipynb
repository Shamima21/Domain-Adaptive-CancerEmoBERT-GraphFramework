{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086e9e9-e913-482b-9cec-233614a9953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b5ca8-15ef-466c-a69a-6bdd85b9b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy = pd.read_csv('D:/Backup/A-collection-27-08-2024/AL_ML_data Science with python/Dataset/CancerEMO/Joy_anon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae2ed0-bd74-48c7-a2e3-8f16a6cdcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edab7fc-8b5b-462f-bd84-fd520ee2a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class PubMedBERTEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "   \n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.squeeze().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8e0b8-9b5c-4ed9-96e9-3b2d3a19d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15be0f-6510-418f-b7ad-356e995fb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('D:/oken1961.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=20, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d015ea-c835-40fe-b265-31ba336158d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = PubMedBERTEmbeddings()\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,  \n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73a9d2-a206-4cf6-a406-8b0ead025d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": 4}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceb4bf-f1a6-4846-811b-8ddad09d10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query):\n",
    "    result = retriever.invoke(query)\n",
    "    context = [doc.page_content for doc in result]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6944584-33a2-4f93-a98c-2d3cdf604459",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_joy['Sentence'].astype(str).tolist()\n",
    "\n",
    "contexts = []\n",
    "for sentence in sentences:\n",
    "    contexts.append(retrieve_context(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3702d5d-a5fa-445e-b5f7-df20f945b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "REPLACE_BY_SPACE = re.compile('[/(){}\\[\\]\\|,;&-_]=')\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"would not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = REPLACE_BY_SPACE.sub(' ', text)\n",
    "    text = [word.strip() for word in text.split()]\n",
    "    text = [re.sub(r'[^\\u0020-\\u007F]+', '', sentence) for sentence in text]\n",
    "    text = [word for word in text if len(word)>2]\n",
    "    text = [word for word in text if word!='amp']\n",
    "    #text = [word.split() for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc538ca0-23e0-4617-90d0-911f61cd8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy['context'] = contexts\n",
    "df_joy['context'] = df_joy['context'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6bd0-3343-43a3-8a34-295366ee0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bcd3f-bc33-4faa-a44e-4893689061e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy['context'] = df_joy['context'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566fb41-6ce7-4012-8ff0-0e69cc87242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy['text'] = (\n",
    "    \"[SENT] \" + df_joy['Sentence'] +\n",
    "    \" [CTX] \" + df_joy['context']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c4000-2c0a-4ad8-9f4b-fc6193180f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_joy[df_joy['Split'] == 0]\n",
    "val_df   = df_joy[df_joy['Split'] == 1]\n",
    "test_df   = df_joy[df_joy['Split'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf17752-078b-4545-b7c0-dbcbb7d63982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize(train_df['text'])\n",
    "val_enc   = tokenize(val_df['text'])\n",
    "test_enc   = tokenize(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28823cb-bd8f-4a48-b709-143d2d56f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "text_input = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_base_en\"\n",
    ")\n",
    "\n",
    "encoder = keras_nlp.models.BertBackbone.from_preset(\n",
    "    \"bert_base_en\"\n",
    ")\n",
    "\n",
    "x = preprocessor(text_input)\n",
    "x = encoder(x)\n",
    "cls = x[\"pooled_output\"]\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(cls)\n",
    "\n",
    "model = tf.keras.Model(text_input, output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0eb29d-5a58-48bb-9e57-1c99d72a7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e455238-0589-4ca3-9dca-cd667dcdb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JoyDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"Joy\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d08c62-2260-4e9b-b53c-720b3f2e18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_df = df_joy[df_joy[\"Split\"] == 0]\n",
    "val_df   = df_joy[df_joy[\"Split\"] == 1]\n",
    "\n",
    "train_ds = JoyDataset(train_df, tokenizer)\n",
    "val_ds   = JoyDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85362bfb-c380-4bb3-a235-c3c309f26397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad3285-4ce9-421f-860f-b364be5e1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "model = BertClassifier(n_classes=2).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1d5a8-8687-4739-a6cf-e9bcdc4ee3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdd55e-3355-4473-b5af-0834830e4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_acc = eval_epoch(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b5957-1a4b-4f2c-a9cd-2e9180032fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
