# -*- coding: utf-8 -*-
"""classic_baselines .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19O9ZfYtZ6MSnCHyQH2rfBDWhs9eWs7nI
"""

import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# -------------------------
# CONFIG (edit these two lines only)
# -------------------------
CSV_PATH  = "/content/sample_data/Surprise_anon.csv"
LABEL_COL = "Surprise"   # must match the dataset

TEXT_COL  = "Sentence"
SPLIT_COL = "Split"      # 0=train, 1=val, 2=test
SEED = 42

def compute_metrics(y_true, y_pred):
    return {
        "acc": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "f1": f1_score(y_true, y_pred, zero_division=0),
    }

df = pd.read_csv(CSV_PATH)
print("Columns:", df.columns.tolist())

df = df[[TEXT_COL, LABEL_COL, SPLIT_COL]].dropna().reset_index(drop=True)

train_df = df[df[SPLIT_COL] == 0]
val_df   = df[df[SPLIT_COL] == 1]
test_df  = df[df[SPLIT_COL] == 2]

X_train = train_df[TEXT_COL].astype(str).tolist()
y_train = train_df[LABEL_COL].astype(int).tolist()
X_val   = val_df[TEXT_COL].astype(str).tolist()
y_val   = val_df[LABEL_COL].astype(int).tolist()
X_test  = test_df[TEXT_COL].astype(str).tolist()
y_test  = test_df[LABEL_COL].astype(int).tolist()

print("\nDATASET:", LABEL_COL)
print("Total:", len(df))
print("Train/Val/Test:", len(X_train), len(X_val), len(X_test))
print("Train label counts:", pd.Series(y_train).value_counts().to_dict())

tfidf = TfidfVectorizer(
    lowercase=True,
    ngram_range=(1, 2),
    min_df=2,
    max_features=50000,
    sublinear_tf=True
)

models = {
    "TFIDF+LogReg": Pipeline([
        ("tfidf", tfidf),
        ("clf", LogisticRegression(
            max_iter=2000,
            class_weight="balanced",
            random_state=SEED
        ))
    ]),
    "TFIDF+LinearSVM": Pipeline([
        ("tfidf", tfidf),
        ("clf", LinearSVC(
            class_weight="balanced",
            random_state=SEED
        ))
    ]),
    "TFIDF+NaiveBayes": Pipeline([
        ("tfidf", tfidf),
        ("clf", MultinomialNB())
    ]),
    "TFIDF+RandomForest": Pipeline([
        ("tfidf", tfidf),
        ("clf", RandomForestClassifier(
            n_estimators=400,
            random_state=SEED,
            n_jobs=-1
        ))
    ]),
}

results = []
for name, model in models.items():
    model.fit(X_train, y_train)

    val_pred  = model.predict(X_val)
    test_pred = model.predict(X_test)

    val_m  = compute_metrics(y_val, val_pred)
    test_m = compute_metrics(y_test, test_pred)

    results.append({
        "model": name,
        "val_acc": val_m["acc"],
        "val_f1": val_m["f1"],
        "val_precision": val_m["precision"],
        "val_recall": val_m["recall"],
        "test_acc": test_m["acc"],
        "test_f1": test_m["f1"],
        "test_precision": test_m["precision"],
        "test_recall": test_m["recall"],
    })

    print(f"{name:18s} | TEST acc={test_m['acc']:.4f} "
          f"f1={test_m['f1']:.4f} prec={test_m['precision']:.4f} rec={test_m['recall']:.4f}")

res_df = pd.DataFrame(results).sort_values("test_f1", ascending=False).reset_index(drop=True)
print("\n================ BASELINE SUMMARY ================")
display(res_df)

out_csv = f"classic_baselines_{LABEL_COL.lower()}.csv"
res_df.to_csv(out_csv, index=False)
print("Saved:", out_csv)

import os
import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ============================================================
# âœ… Dynamic TF-IDF Baselines Runner (ALL datasets in one run)
# - Assumes each CSV has columns: Sentence, <LABEL>, Split (0/1/2)
# - Saves:
#   1) classic_baselines_ALL.csv (summary across all datasets)
#   2) classic_baselines_<label>.csv (per-dataset summary)
# ============================================================

# -------------------------
# CONFIG (edit only this block)
# -------------------------
DATA_DIR  = "/content/sample_data"  # folder containing all your *_anon.csv files
TEXT_COL  = "Sentence"
SPLIT_COL = "Split"      # 0=train, 1=val, 2=test
SEED = 42

# Option A: list your datasets explicitly (recommended)
DATASETS = [
    {"label": "Surprise",  "csv": "Surprise_anon.csv"},
    {"label": "Sadness",   "csv": "Sadness_anon.csv"},
    {"label": "Joy",       "csv": "Joy_anon.csv"},
    {"label": "Fear",      "csv": "Fear_anon.csv"},
    {"label": "Anger",     "csv": "Anger_anon.csv"},
    {"label": "Disgust",   "csv": "Disgust_anon.csv"},
    {"label": "Trust",     "csv": "Trust_anon.csv"},
    {"label": "Anticipation",     "csv": "Anticipation_anon.csv"},
]
# -------------------------


def compute_metrics(y_true, y_pred):
    return {
        "acc": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "f1": f1_score(y_true, y_pred, zero_division=0),
    }


def build_models(seed=42):
    # fresh TF-IDF object per model (safe)
    def make_tfidf():
        return TfidfVectorizer(
            lowercase=True,
            ngram_range=(1, 2),
            min_df=2,
            max_features=50000,
            sublinear_tf=True
        )

    return {
        "TFIDF+LogReg": Pipeline([
            ("tfidf", make_tfidf()),
            ("clf", LogisticRegression(
                max_iter=2000,
                class_weight="balanced",
                random_state=seed
            ))
        ]),
        "TFIDF+LinearSVM": Pipeline([
            ("tfidf", make_tfidf()),
            ("clf", LinearSVC(
                class_weight="balanced",
                random_state=seed
            ))
        ]),
        "TFIDF+NaiveBayes": Pipeline([
            ("tfidf", make_tfidf()),
            ("clf", MultinomialNB())
        ]),
        "TFIDF+RandomForest": Pipeline([
            ("tfidf", make_tfidf()),
            ("clf", RandomForestClassifier(
                n_estimators=400,
                random_state=seed,
                n_jobs=-1
            ))
        ]),
    }


def run_one_dataset(csv_path, label_col, text_col="Sentence", split_col="Split", seed=42):
    df = pd.read_csv(csv_path)
    required = {text_col, label_col, split_col}
    missing = sorted(list(required - set(df.columns)))
    if missing:
        raise ValueError(f"Missing columns in {os.path.basename(csv_path)}: {missing}")

    df = df[[text_col, label_col, split_col]].dropna().reset_index(drop=True)

    train_df = df[df[split_col] == 0]
    val_df   = df[df[split_col] == 1]
    test_df  = df[df[split_col] == 2]

    X_train = train_df[text_col].astype(str).tolist()
    y_train = train_df[label_col].astype(int).tolist()
    X_val   = val_df[text_col].astype(str).tolist()
    y_val   = val_df[label_col].astype(int).tolist()
    X_test  = test_df[text_col].astype(str).tolist()
    y_test  = test_df[label_col].astype(int).tolist()

    print("\n================================================")
    print("DATASET:", label_col)
    print("FILE   :", os.path.basename(csv_path))
    print("Total  :", len(df))
    print("Train/Val/Test:", len(X_train), len(X_val), len(X_test))
    print("Train label counts:", pd.Series(y_train).value_counts().to_dict())

    models = build_models(seed)
    rows = []

    for name, model in models.items():
        model.fit(X_train, y_train)

        val_pred  = model.predict(X_val)  if len(X_val)  > 0 else []
        test_pred = model.predict(X_test) if len(X_test) > 0 else []

        val_m  = compute_metrics(y_val, val_pred)   if len(X_val)  > 0 else {"acc": np.nan, "f1": np.nan, "precision": np.nan, "recall": np.nan}
        test_m = compute_metrics(y_test, test_pred) if len(X_test) > 0 else {"acc": np.nan, "f1": np.nan, "precision": np.nan, "recall": np.nan}

        rows.append({
            "dataset": label_col,
            "model": name,
            "val_acc": val_m["acc"],
            "val_f1": val_m["f1"],
            "val_precision": val_m["precision"],
            "val_recall": val_m["recall"],
            "test_acc": test_m["acc"],
            "test_f1": test_m["f1"],
            "test_precision": test_m["precision"],
            "test_recall": test_m["recall"],
        })

        print(f"{name:18s} | TEST acc={test_m['acc']:.4f} "
              f"f1={test_m['f1']:.4f} prec={test_m['precision']:.4f} rec={test_m['recall']:.4f}")

    res_df = pd.DataFrame(rows).sort_values("test_f1", ascending=False).reset_index(drop=True)
    return res_df


# -------------------------
# RUN ALL DATASETS
# -------------------------
all_results = []
for ds in DATASETS:
    label = ds["label"]
    csv_path = os.path.join(DATA_DIR, ds["csv"])

    if not os.path.exists(csv_path):
        print(f"\n[SKIP] File not found: {csv_path}")
        continue

    res_df = run_one_dataset(csv_path, label_col=label, text_col=TEXT_COL, split_col=SPLIT_COL, seed=SEED)
    all_results.append(res_df)

    out_csv = f"classic_baselines_{label.lower()}.csv"
    res_df.to_csv(out_csv, index=False)
    print("Saved:", out_csv)

if len(all_results) == 0:
    raise RuntimeError("No datasets were run. Check DATA_DIR and DATASETS filenames.")

all_df = pd.concat(all_results, ignore_index=True)

# nicer summary: best model per dataset (by test_f1)
best_per_dataset = (
    all_df.sort_values(["dataset", "test_f1"], ascending=[True, False])
         .groupby("dataset", as_index=False)
         .head(1)
         .reset_index(drop=True)
)

print("\n================ ALL DATASETS: FULL RESULTS ================")
display(all_df.sort_values(["dataset", "test_f1"], ascending=[True, False]).reset_index(drop=True))

print("\n================ BEST PER DATASET (by TEST F1) ================")
display(best_per_dataset)

all_out = "classic_baselines_ALL.csv"
best_out = "classic_baselines_BEST_PER_DATASET.csv"
all_df.to_csv(all_out, index=False)
best_per_dataset.to_csv(best_out, index=False)
print("Saved:", all_out)
print("Saved:", best_out)